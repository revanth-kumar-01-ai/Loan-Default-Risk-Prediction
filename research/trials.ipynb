{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.chdir('../')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity \n",
    "\n",
    "from dataclasses import dataclass \n",
    "from pathlib import Path \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_query: str  \n",
    "    load_data: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration manager in src config\n",
    "\n",
    "import warnings \n",
    "\n",
    "from loan_default_risk.constants import * \n",
    "from loan_default_risk.utils.common import read_yaml, create_directories \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    # data ingestion ðŸ’‰\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_query=config.source_query,\n",
    "            load_data=config.load_data,\n",
    "        )\n",
    "\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components \n",
    "import os\n",
    "from sqlalchemy import create_engine, text \n",
    "from urllib.parse import quote\n",
    "from loan_default_risk import logger\n",
    "from loan_default_risk.utils.common import get_size\n",
    "from ensure import ensure_annotations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def fetch_data_from_database(self):\n",
    "        if not os.path.exists(self.config.load_data):\n",
    "\n",
    "            engine = create_engine(f\"mysql+pymysql://root:{quote('Reva@0411')}@localhost/loan_credit\")\n",
    "            sql = text(self.config.source_query)\n",
    "            with engine.connect() as connection:\n",
    "                result = connection.execute(sql)\n",
    "                df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "        \n",
    "            df.to_csv(self.config.load_data, index=False)\n",
    "            logger.info(f\"{self.config.load_data} downloaded! with following info: loanDataset.csv\")\n",
    "        else:\n",
    "            logger.info(f\"File already exists of size: {get_size(Path(self.config.load_data))}\")\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-28 23:41:25,292: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-02-28 23:41:25,293: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-02-28 23:41:25,294: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-02-28 23:41:25,294: INFO: common: created directory at: artifacts]\n",
      "[2025-02-28 23:41:25,295: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2025-02-28 23:41:25,296: INFO: 4108454040: File already exists of size: ~ 94 KB]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.fetch_data_from_database()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessing_model:\n",
    "#   root_dir: artifacts/data_preprocessing\n",
    "#   columnTransferPipeline: artifacts/data_preprocessing/Preprocessing_pipeline.joblib\n",
    "#   outliersPipeline: artifacts/data_preprocessing/outliers_pipeline.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity \n",
    "\n",
    "from dataclasses import dataclass \n",
    "from pathlib import Path \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreProcessingConfig:\n",
    "    root_dir: Path\n",
    "    columnTransferPipeline: Path\n",
    "    outliersPipeline: Path\n",
    "    cleanDataset: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration manager in src config\n",
    "\n",
    "import warnings \n",
    "\n",
    "from loan_default_risk.constants import * \n",
    "from loan_default_risk.utils.common import read_yaml, create_directories \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    # data preprocessing ðŸ’‰\n",
    "    def get_data_preprocessing_config(self) -> DataPreProcessingConfig:\n",
    "        config = self.config.data_preprocessing_model\n",
    "\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "\n",
    "        data_preprocessing_config = DataPreProcessingConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            columnTransferPipeline=config.columnTransferPipeline,\n",
    "            outliersPipeline=config.outliersPipeline,\n",
    "            cleanDataset = config.cleanDataset\n",
    "        )\n",
    "\n",
    "\n",
    "        return data_preprocessing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from pathlib import Path\n",
    "from loan_default_risk import logger\n",
    "from loan_default_risk.utils.common import get_size\n",
    "from feature_engine.outliers import Winsorizer\n",
    "\n",
    "\n",
    "# df = pd.read_csv('artifacts/data_ingestion/loanDataset.csv')\n",
    "class DataPreprocessing:\n",
    "    def __init__(self, config: DataPreProcessingConfig):\n",
    "        self.config = config\n",
    "        self.df = pd.read_csv('artifacts/data_ingestion/loanDataset.csv')\n",
    "\n",
    "    \n",
    "    def create_column_transfer_pipeline(self):\n",
    "        try: \n",
    "            # Numeric features\n",
    "            numeric_features = df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "            # Categorical features\n",
    "            categorical_features = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "            # Check if the pipeline file already exists\n",
    "            if not os.path.exists(self.config.columnTransferPipeline):\n",
    "                # Create numeric pipeline\n",
    "                num_pipeLine = Pipeline(steps=[\n",
    "                    ('impute', SimpleImputer(strategy='mean')),\n",
    "                    ('Scale', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "                # Create categorical pipeline\n",
    "                encoding_pipeline = Pipeline([\n",
    "                    ('oneHotEncode', OneHotEncoder(sparse_output=False))\n",
    "                ])\n",
    "\n",
    "                # Combine numeric and categorical pipelines\n",
    "                preprocess_pipeline = ColumnTransformer([\n",
    "                    ('numeric', num_pipeLine, numeric_features),\n",
    "                    ('categorical', encoding_pipeline, categorical_features)\n",
    "                ])\n",
    "\n",
    "                # Save the pipeline to a file\n",
    "                joblib.dump(preprocess_pipeline, self.config.columnTransferPipeline)\n",
    "                logger.info(f\"Pipeline saved at: {self.config.columnTransferPipeline}\")\n",
    "            else:\n",
    "                logger.info(f\"File already exists of size: {get_size(Path(self.config.columnTransferPipeline))}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log any exceptions that occur\n",
    "            logger.error(f\"Error occurred during pipeline creation: {e}\")\n",
    "            raise e \n",
    "        \n",
    "    def outliersPipeline(self):\n",
    "            try:\n",
    "                # Check if the outliers pipeline file already exists\n",
    "                if not os.path.exists(self.config.outliersPipeline):\n",
    "                    # Define features to handle outliers\n",
    "                    outliersFeatures = [\n",
    "                        'numeric__months_loan_duration',\n",
    "                        'numeric__amount',\n",
    "                        'numeric__age'\n",
    "                    ]\n",
    "\n",
    "                    # Create Winsorizer object\n",
    "                    winsor = Winsorizer(\n",
    "                        capping_method='iqr',  # Use IQR rule boundaries\n",
    "                        tail='both',  # Cap both tails\n",
    "                        fold=1.5,  # Fold value for IQR\n",
    "                        variables=outliersFeatures\n",
    "                    )\n",
    "\n",
    "                    # Save the Winsorizer pipeline to a file\n",
    "                    joblib.dump(winsor, self.config.outliersPipeline)\n",
    "                    logger.info(f\"Outliers pipeline saved at: {self.config.outliersPipeline}\")\n",
    "                else:\n",
    "                    logger.info(f\"File already exists of size: {get_size(Path(self.config.outliersPipeline))}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log any exceptions that occur\n",
    "                logger.error(f\"Error occurred during outliers pipeline creation: {e}\")\n",
    "                raise e  # Re-raise the exception if needed\n",
    "            \n",
    "    \n",
    "    def preprocessingDataset(self):\n",
    "        try:\n",
    "            df = self.df\n",
    "            # Load the preprocessing pipeline\n",
    "            preprocess = joblib.load(self.config.columnTransferPipeline)\n",
    "\n",
    "            # Load the outliers pipeline\n",
    "            outlier = joblib.load(self.config.outliersPipeline)\n",
    "\n",
    "            # Apply the preprocessing pipeline to the dataset\n",
    "            df = pd.DataFrame(\n",
    "                preprocess.fit_transform(df),\n",
    "                columns=preprocess.get_feature_names_out()\n",
    "            )\n",
    "\n",
    "            # Apply the outliers pipeline to specific numeric features\n",
    "            df[['numeric__months_loan_duration', 'numeric__amount', 'numeric__age']] = outlier.fit_transform(\n",
    "                df[['numeric__months_loan_duration', 'numeric__amount', 'numeric__age']]\n",
    "            )\n",
    "\n",
    "            # Save the cleaned dataset to a CSV file\n",
    "            if not os.path.exists(self.config.cleanDataset):\n",
    "                df.to_csv(self.config.cleanDataset, index=False)\n",
    "                logger.info(f\"{self.config.cleanDataset} saved! with following info: cleanDataset.csv\")\n",
    "            else:\n",
    "                logger.info(f\"File already exists of size: {get_size(Path(self.config.cleanDataset))}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log any exceptions that occur\n",
    "            logger.error(f\"Error occurred during dataset preprocessing: {e}\")\n",
    "            raise e  # Re-raise the exception if needed\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-01 01:30:35,769: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-01 01:30:35,770: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-01 01:30:35,771: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-03-01 01:30:35,772: INFO: common: created directory at: artifacts]\n",
      "[2025-03-01 01:30:35,772: INFO: common: created directory at: artifacts/data_preprocessing]\n",
      "[2025-03-01 01:30:35,778: INFO: 2538319982: Pipeline saved at: artifacts/data_preprocessing/Preprocessing_pipeline.joblib]\n",
      "[2025-03-01 01:30:35,779: INFO: 2538319982: Outliers pipeline saved at: artifacts/data_preprocessing/outliers_pipeline.joblib]\n",
      "[2025-03-01 01:30:35,816: INFO: 2538319982: artifacts/data_preprocessing/preprocessedDataset.csv saved! with following info: cleanDataset.csv]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_preprocessing_config = config.get_data_preprocessing_config()\n",
    "    data_preprocess = DataPreprocessing(config=data_preprocessing_config)\n",
    "    data_preprocess.create_column_transfer_pipeline()\n",
    "    data_preprocess.outliersPipeline()\n",
    "    data_preprocess.preprocessingDataset()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
